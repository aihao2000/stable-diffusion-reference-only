{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "from diffusers import UNet2DConditionModel\n",
    "from PIL import Image\n",
    "import sys\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "sys.path.append(\"/home/aihao/workspace/StableDiffusionReferenceOnly/src\")\n",
    "from stable_diffusion_joint_control.pipelines.stable_diffusion_reference_only_pipeline import (\n",
    "    StableDiffusionReferenceOnlyPipeline,\n",
    ")\n",
    "from stable_diffusion_joint_control.models.dobule_condition_unet import (\n",
    "    UNet2DDobuleConditionModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa73015b7fe41f8a1cf3dd8d3a41200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sd_pipe=diffusers.StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = dict(sd_pipe.unet.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_size': 96,\n",
       " 'in_channels': 4,\n",
       " 'out_channels': 4,\n",
       " 'center_input_sample': False,\n",
       " 'flip_sin_to_cos': True,\n",
       " 'freq_shift': 0,\n",
       " 'down_block_types': ['CrossAttnDownBlock2D',\n",
       "  'CrossAttnDownBlock2D',\n",
       "  'CrossAttnDownBlock2D',\n",
       "  'DownBlock2D'],\n",
       " 'mid_block_type': 'UNetMidBlock2DCrossAttn',\n",
       " 'up_block_types': ['UpBlock2D',\n",
       "  'CrossAttnUpBlock2D',\n",
       "  'CrossAttnUpBlock2D',\n",
       "  'CrossAttnUpBlock2D'],\n",
       " 'only_cross_attention': False,\n",
       " 'block_out_channels': [320, 640, 1280, 1280],\n",
       " 'layers_per_block': 2,\n",
       " 'downsample_padding': 1,\n",
       " 'mid_block_scale_factor': 1,\n",
       " 'act_fn': 'silu',\n",
       " 'norm_num_groups': 32,\n",
       " 'norm_eps': 1e-05,\n",
       " 'cross_attention_dim': 1024,\n",
       " 'transformer_layers_per_block': 1,\n",
       " 'encoder_hid_dim': None,\n",
       " 'encoder_hid_dim_type': None,\n",
       " 'attention_head_dim': [5, 10, 20, 20],\n",
       " 'num_attention_heads': None,\n",
       " 'dual_cross_attention': False,\n",
       " 'use_linear_projection': True,\n",
       " 'class_embed_type': None,\n",
       " 'addition_embed_type': None,\n",
       " 'addition_time_embed_dim': None,\n",
       " 'num_class_embeds': None,\n",
       " 'upcast_attention': True,\n",
       " 'resnet_time_scale_shift': 'default',\n",
       " 'resnet_skip_time_act': False,\n",
       " 'resnet_out_scale_factor': 1.0,\n",
       " 'time_embedding_type': 'positional',\n",
       " 'time_embedding_dim': None,\n",
       " 'time_embedding_act_fn': None,\n",
       " 'timestep_post_act': None,\n",
       " 'time_cond_proj_dim': None,\n",
       " 'conv_in_kernel': 3,\n",
       " 'conv_out_kernel': 3,\n",
       " 'projection_class_embeddings_input_dim': None,\n",
       " 'attention_type': 'default',\n",
       " 'class_embeddings_concat': False,\n",
       " 'mid_block_only_cross_attention': None,\n",
       " 'cross_attention_norm': None,\n",
       " 'addition_embed_type_num_heads': 64,\n",
       " '_use_default_values': ['resnet_skip_time_act',\n",
       "  'num_attention_heads',\n",
       "  'addition_embed_type_num_heads',\n",
       "  'time_embedding_type',\n",
       "  'mid_block_type',\n",
       "  'cross_attention_norm',\n",
       "  'time_cond_proj_dim',\n",
       "  'addition_time_embed_dim',\n",
       "  'time_embedding_act_fn',\n",
       "  'projection_class_embeddings_input_dim',\n",
       "  'encoder_hid_dim_type',\n",
       "  'conv_out_kernel',\n",
       "  'attention_type',\n",
       "  'time_embedding_dim',\n",
       "  'class_embeddings_concat',\n",
       "  'conv_in_kernel',\n",
       "  'transformer_layers_per_block',\n",
       "  'timestep_post_act',\n",
       "  'resnet_out_scale_factor',\n",
       "  'encoder_hid_dim',\n",
       "  'class_embed_type',\n",
       "  'resnet_time_scale_shift',\n",
       "  'mid_block_only_cross_attention',\n",
       "  'addition_embed_type'],\n",
       " '_class_name': 'UNet2DConditionModel',\n",
       " '_diffusers_version': '0.10.0.dev0',\n",
       " '_name_or_path': '/home/aihao/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config['cross_attention_dim']=1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config[\"_name_or_path\"] = \"unet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_size': 96,\n",
       " 'in_channels': 4,\n",
       " 'out_channels': 4,\n",
       " 'center_input_sample': False,\n",
       " 'flip_sin_to_cos': True,\n",
       " 'freq_shift': 0,\n",
       " 'down_block_types': ['CrossAttnDownBlock2D',\n",
       "  'CrossAttnDownBlock2D',\n",
       "  'CrossAttnDownBlock2D',\n",
       "  'DownBlock2D'],\n",
       " 'mid_block_type': 'UNetMidBlock2DCrossAttn',\n",
       " 'up_block_types': ['UpBlock2D',\n",
       "  'CrossAttnUpBlock2D',\n",
       "  'CrossAttnUpBlock2D',\n",
       "  'CrossAttnUpBlock2D'],\n",
       " 'only_cross_attention': False,\n",
       " 'block_out_channels': [320, 640, 1280, 1280],\n",
       " 'layers_per_block': 2,\n",
       " 'downsample_padding': 1,\n",
       " 'mid_block_scale_factor': 1,\n",
       " 'act_fn': 'silu',\n",
       " 'norm_num_groups': 32,\n",
       " 'norm_eps': 1e-05,\n",
       " 'cross_attention_dim': 1664,\n",
       " 'transformer_layers_per_block': 1,\n",
       " 'encoder_hid_dim': None,\n",
       " 'encoder_hid_dim_type': None,\n",
       " 'attention_head_dim': [5, 10, 20, 20],\n",
       " 'num_attention_heads': None,\n",
       " 'dual_cross_attention': False,\n",
       " 'use_linear_projection': True,\n",
       " 'class_embed_type': None,\n",
       " 'addition_embed_type': None,\n",
       " 'addition_time_embed_dim': None,\n",
       " 'num_class_embeds': None,\n",
       " 'upcast_attention': True,\n",
       " 'resnet_time_scale_shift': 'default',\n",
       " 'resnet_skip_time_act': False,\n",
       " 'resnet_out_scale_factor': 1.0,\n",
       " 'time_embedding_type': 'positional',\n",
       " 'time_embedding_dim': None,\n",
       " 'time_embedding_act_fn': None,\n",
       " 'timestep_post_act': None,\n",
       " 'time_cond_proj_dim': None,\n",
       " 'conv_in_kernel': 3,\n",
       " 'conv_out_kernel': 3,\n",
       " 'projection_class_embeddings_input_dim': None,\n",
       " 'attention_type': 'default',\n",
       " 'class_embeddings_concat': False,\n",
       " 'mid_block_only_cross_attention': None,\n",
       " 'cross_attention_norm': None,\n",
       " 'addition_embed_type_num_heads': 64,\n",
       " '_use_default_values': ['resnet_skip_time_act',\n",
       "  'num_attention_heads',\n",
       "  'addition_embed_type_num_heads',\n",
       "  'time_embedding_type',\n",
       "  'mid_block_type',\n",
       "  'cross_attention_norm',\n",
       "  'time_cond_proj_dim',\n",
       "  'addition_time_embed_dim',\n",
       "  'time_embedding_act_fn',\n",
       "  'projection_class_embeddings_input_dim',\n",
       "  'encoder_hid_dim_type',\n",
       "  'conv_out_kernel',\n",
       "  'attention_type',\n",
       "  'time_embedding_dim',\n",
       "  'class_embeddings_concat',\n",
       "  'conv_in_kernel',\n",
       "  'transformer_layers_per_block',\n",
       "  'timestep_post_act',\n",
       "  'resnet_out_scale_factor',\n",
       "  'encoder_hid_dim',\n",
       "  'class_embed_type',\n",
       "  'resnet_time_scale_shift',\n",
       "  'mid_block_only_cross_attention',\n",
       "  'addition_embed_type'],\n",
       " '_class_name': 'UNet2DConditionModel',\n",
       " '_diffusers_version': '0.10.0.dev0',\n",
       " '_name_or_path': 'unet'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DDobuleConditionModel.from_config(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DDobuleConditionModel(\n",
       "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (controlnet_cond_embedding): ControlNetConditioningEmbedding(\n",
       "    (conv_in): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (blocks): ModuleList(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (conv_out): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1664, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1664, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f077d499d4d4de185661fee00c7796c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_encoder = CLIPVisionModel.from_pretrained(\n",
    "    \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image_processor = CLIPImageProcessor.from_pretrained(\n",
    "    \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = StableDiffusionReferenceOnlyPipeline(\n",
    "    sd_pipe.vae, image_encoder, clip_image_processor, unet, sd_pipe.scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline.save_pretrained(\n",
    "    \"/home/aihao/workspace/DeepLearningContent/models/sd_ro/init\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv_in.weight', tensor([[[[-0.1058,  0.1352,  0.1860],\n",
      "          [-0.1434, -0.0410, -0.0316],\n",
      "          [-0.1231, -0.0663, -0.1309]],\n",
      "\n",
      "         [[-0.0760, -0.0124, -0.1657],\n",
      "          [ 0.1889, -0.0391, -0.0769],\n",
      "          [ 0.0940,  0.1285,  0.0888]],\n",
      "\n",
      "         [[-0.1047, -0.0279,  0.0548],\n",
      "          [ 0.1347, -0.0195, -0.1463],\n",
      "          [-0.1547,  0.1799,  0.0938]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0114,  0.1891,  0.1718],\n",
      "          [-0.1248,  0.1773, -0.1163],\n",
      "          [-0.1482, -0.0503, -0.0398]],\n",
      "\n",
      "         [[ 0.1719, -0.0027,  0.0023],\n",
      "          [ 0.1358, -0.0985, -0.1177],\n",
      "          [ 0.0454, -0.1468,  0.1322]],\n",
      "\n",
      "         [[-0.1124,  0.1706,  0.0172],\n",
      "          [-0.0410,  0.1312, -0.0744],\n",
      "          [-0.1764, -0.0704,  0.1370]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0777, -0.0686,  0.1272],\n",
      "          [-0.0374,  0.0384, -0.0537],\n",
      "          [ 0.1043, -0.1520, -0.1174]],\n",
      "\n",
      "         [[-0.0819, -0.0611,  0.0649],\n",
      "          [-0.1411, -0.0771,  0.0908],\n",
      "          [-0.1658,  0.1142, -0.0889]],\n",
      "\n",
      "         [[-0.1188, -0.1886, -0.1372],\n",
      "          [ 0.1180, -0.0768, -0.0266],\n",
      "          [-0.1808, -0.1047, -0.1464]]],\n",
      "\n",
      "\n",
      "        [[[-0.1572, -0.0365,  0.1143],\n",
      "          [ 0.0082, -0.1172,  0.1558],\n",
      "          [ 0.0952, -0.0466, -0.1124]],\n",
      "\n",
      "         [[ 0.0149, -0.0128, -0.1749],\n",
      "          [-0.1005,  0.0874, -0.1511],\n",
      "          [-0.0005, -0.1556, -0.0671]],\n",
      "\n",
      "         [[ 0.1897, -0.0633,  0.1263],\n",
      "          [-0.1724,  0.0594,  0.0643],\n",
      "          [ 0.1042, -0.0346, -0.0647]]],\n",
      "\n",
      "\n",
      "        [[[-0.1390, -0.0698,  0.0938],\n",
      "          [-0.0220,  0.0069,  0.0863],\n",
      "          [-0.1318, -0.0932,  0.0229]],\n",
      "\n",
      "         [[-0.1520, -0.0601,  0.1130],\n",
      "          [ 0.0917, -0.0140, -0.0726],\n",
      "          [-0.0268, -0.0252, -0.1628]],\n",
      "\n",
      "         [[-0.1388,  0.1324,  0.1844],\n",
      "          [ 0.0627,  0.1711,  0.1431],\n",
      "          [-0.1770, -0.0783,  0.1106]]],\n",
      "\n",
      "\n",
      "        [[[-0.1874, -0.0173, -0.1824],\n",
      "          [ 0.1111, -0.1148,  0.1313],\n",
      "          [-0.0461, -0.1065,  0.0250]],\n",
      "\n",
      "         [[ 0.1693, -0.0332, -0.0008],\n",
      "          [ 0.1130,  0.0184, -0.1807],\n",
      "          [ 0.1075,  0.0139,  0.0278]],\n",
      "\n",
      "         [[ 0.0341,  0.1641, -0.1407],\n",
      "          [-0.0904, -0.1749, -0.1540],\n",
      "          [ 0.1783,  0.0921, -0.0717]]],\n",
      "\n",
      "\n",
      "        [[[-0.1081,  0.0775,  0.1547],\n",
      "          [ 0.0931,  0.1140,  0.0479],\n",
      "          [-0.0421,  0.1169, -0.1500]],\n",
      "\n",
      "         [[ 0.1456, -0.0893,  0.1766],\n",
      "          [-0.0053,  0.1313, -0.1184],\n",
      "          [ 0.0356,  0.1184,  0.0612]],\n",
      "\n",
      "         [[ 0.1438,  0.0329,  0.0453],\n",
      "          [-0.1868, -0.1285, -0.0242],\n",
      "          [-0.1352,  0.1018,  0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.1727,  0.1590,  0.1628],\n",
      "          [-0.0495,  0.0326,  0.0636],\n",
      "          [-0.0307,  0.1324, -0.0769]],\n",
      "\n",
      "         [[ 0.0987,  0.0827,  0.0760],\n",
      "          [-0.1858, -0.0119,  0.1687],\n",
      "          [-0.1851,  0.0013, -0.0106]],\n",
      "\n",
      "         [[-0.0762,  0.0666, -0.0068],\n",
      "          [-0.0030, -0.1559, -0.0300],\n",
      "          [ 0.0319,  0.1805,  0.1802]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1676,  0.0375, -0.0693],\n",
      "          [-0.1106,  0.0375, -0.0616],\n",
      "          [ 0.1319, -0.1863,  0.1406]],\n",
      "\n",
      "         [[-0.0338, -0.1307,  0.1005],\n",
      "          [ 0.1257,  0.0671,  0.0121],\n",
      "          [-0.0610, -0.0212,  0.1838]],\n",
      "\n",
      "         [[-0.1082,  0.0922, -0.0841],\n",
      "          [ 0.1422,  0.0463, -0.0382],\n",
      "          [-0.0496, -0.1131, -0.0857]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0332, -0.0325,  0.0544],\n",
      "          [-0.1377,  0.1176, -0.1097],\n",
      "          [ 0.1682,  0.1471, -0.1316]],\n",
      "\n",
      "         [[ 0.0724,  0.0372,  0.1875],\n",
      "          [ 0.0887,  0.0540, -0.1532],\n",
      "          [ 0.1730,  0.0787,  0.0726]],\n",
      "\n",
      "         [[ 0.0756, -0.1293,  0.0394],\n",
      "          [ 0.0421,  0.0553,  0.0446],\n",
      "          [ 0.0130, -0.0635, -0.1679]]],\n",
      "\n",
      "\n",
      "        [[[-0.1073,  0.0830, -0.0639],\n",
      "          [-0.0504,  0.1304,  0.1894],\n",
      "          [-0.1461,  0.0495, -0.1270]],\n",
      "\n",
      "         [[-0.0748,  0.1440,  0.1044],\n",
      "          [-0.0533,  0.0835, -0.1068],\n",
      "          [-0.1415,  0.1359,  0.0847]],\n",
      "\n",
      "         [[ 0.0995,  0.0565, -0.1891],\n",
      "          [-0.1225,  0.0882, -0.0997],\n",
      "          [ 0.0305, -0.0468,  0.0359]]],\n",
      "\n",
      "\n",
      "        [[[-0.0314,  0.0564, -0.1097],\n",
      "          [ 0.0122, -0.0159, -0.1854],\n",
      "          [ 0.1186,  0.0276,  0.1716]],\n",
      "\n",
      "         [[-0.1572, -0.0674, -0.0515],\n",
      "          [ 0.1919,  0.1599,  0.0276],\n",
      "          [-0.1380, -0.0361,  0.1125]],\n",
      "\n",
      "         [[ 0.0524, -0.1111, -0.1546],\n",
      "          [-0.0765, -0.1613,  0.1922],\n",
      "          [ 0.0991,  0.1501,  0.1331]]],\n",
      "\n",
      "\n",
      "        [[[-0.0239,  0.0275, -0.0175],\n",
      "          [ 0.1368,  0.1090,  0.1781],\n",
      "          [ 0.1100,  0.1120, -0.1381]],\n",
      "\n",
      "         [[ 0.0476, -0.0769,  0.1734],\n",
      "          [-0.0133,  0.0746, -0.0034],\n",
      "          [-0.0156,  0.0179, -0.0839]],\n",
      "\n",
      "         [[-0.1869,  0.0530, -0.0887],\n",
      "          [-0.1441,  0.0599,  0.1578],\n",
      "          [ 0.1634, -0.1539, -0.0649]]],\n",
      "\n",
      "\n",
      "        [[[-0.1125, -0.0362,  0.0426],\n",
      "          [ 0.0876, -0.1913,  0.0570],\n",
      "          [-0.0606,  0.0577, -0.1645]],\n",
      "\n",
      "         [[-0.1900, -0.0121, -0.1893],\n",
      "          [ 0.0976,  0.1639, -0.1384],\n",
      "          [-0.1103,  0.0609, -0.0167]],\n",
      "\n",
      "         [[-0.0897,  0.0334,  0.0979],\n",
      "          [ 0.0183,  0.0097,  0.0342],\n",
      "          [-0.0940,  0.1537,  0.1146]]],\n",
      "\n",
      "\n",
      "        [[[-0.0531, -0.1423,  0.0479],\n",
      "          [ 0.1879, -0.0346,  0.0631],\n",
      "          [-0.0976,  0.0387, -0.1387]],\n",
      "\n",
      "         [[ 0.0373, -0.1860, -0.0560],\n",
      "          [ 0.1856, -0.0037, -0.1329],\n",
      "          [-0.0436,  0.1158, -0.1755]],\n",
      "\n",
      "         [[ 0.0256,  0.1271,  0.1401],\n",
      "          [-0.1562,  0.0970, -0.0746],\n",
      "          [-0.0485, -0.0530,  0.1059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0798, -0.0648, -0.0898],\n",
      "          [-0.1794, -0.0097,  0.0945],\n",
      "          [-0.1666,  0.1220, -0.0084]],\n",
      "\n",
      "         [[ 0.0872,  0.1909, -0.1191],\n",
      "          [-0.0018,  0.0862, -0.0999],\n",
      "          [ 0.1645, -0.1607,  0.1512]],\n",
      "\n",
      "         [[-0.0364, -0.1889,  0.0467],\n",
      "          [ 0.1826, -0.1654,  0.0575],\n",
      "          [ 0.1323,  0.1140, -0.0514]]]])), ('conv_in.bias', tensor([-0.0885, -0.1652,  0.1531,  0.0416, -0.1449,  0.1659,  0.1908, -0.0963,\n",
      "        -0.0980,  0.0472, -0.0577, -0.1158, -0.1540,  0.1613,  0.1513,  0.1225])), ('blocks.0.weight', tensor([[[[-4.6171e-02,  1.6496e-02, -7.6817e-02],\n",
      "          [-1.5379e-02,  5.8066e-02, -5.7889e-02],\n",
      "          [-4.0549e-02,  5.5214e-02,  5.0760e-02]],\n",
      "\n",
      "         [[-4.7657e-02, -3.6786e-02, -5.4178e-02],\n",
      "          [ 7.5809e-02,  4.4175e-02, -6.3537e-02],\n",
      "          [ 7.3131e-03,  6.5296e-02, -2.5316e-02]],\n",
      "\n",
      "         [[-7.1115e-03, -1.7244e-02, -7.9941e-02],\n",
      "          [ 7.4645e-02, -3.9210e-03,  6.7679e-02],\n",
      "          [-2.1690e-02,  7.5119e-02, -8.0839e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6982e-02,  7.6991e-02, -1.3709e-02],\n",
      "          [-6.7251e-02,  7.9946e-02, -3.7321e-02],\n",
      "          [-6.5889e-02, -7.5948e-02,  3.8474e-02]],\n",
      "\n",
      "         [[ 8.4537e-03, -5.7927e-02, -1.2606e-02],\n",
      "          [-6.0620e-02, -5.6339e-02,  7.7859e-02],\n",
      "          [-6.9745e-03, -4.9960e-02,  2.3514e-02]],\n",
      "\n",
      "         [[-4.4021e-02, -6.0969e-03,  3.8073e-02],\n",
      "          [-5.8523e-02,  2.7821e-02, -1.3483e-02],\n",
      "          [-3.6535e-02,  8.6380e-03,  8.1289e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.9059e-02,  2.7075e-02,  2.8279e-02],\n",
      "          [ 4.2917e-02,  5.4704e-02, -6.8428e-02],\n",
      "          [-5.0002e-03,  2.5890e-02,  8.6990e-03]],\n",
      "\n",
      "         [[ 5.8365e-02, -1.0439e-02,  1.0275e-02],\n",
      "          [-5.7751e-02, -3.3784e-02, -7.7896e-03],\n",
      "          [ 1.8600e-02,  2.1123e-03, -7.7151e-02]],\n",
      "\n",
      "         [[-7.1919e-04,  6.2252e-02, -4.0421e-02],\n",
      "          [-2.2961e-02,  3.0928e-03,  2.3057e-03],\n",
      "          [-2.1740e-02,  4.9953e-02, -5.1647e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7324e-02, -4.5922e-02, -5.9739e-03],\n",
      "          [ 6.8224e-02, -1.1724e-02, -5.1567e-02],\n",
      "          [ 5.1413e-02,  4.3171e-02,  6.1358e-02]],\n",
      "\n",
      "         [[-4.6571e-02, -7.9001e-02,  6.7564e-02],\n",
      "          [-6.6890e-02,  5.0846e-02,  4.7378e-02],\n",
      "          [-5.1169e-02,  8.1165e-02, -4.4056e-02]],\n",
      "\n",
      "         [[-5.9446e-02,  5.1323e-02, -6.2924e-02],\n",
      "          [-5.6122e-02,  2.6625e-02, -2.1368e-02],\n",
      "          [ 5.8532e-02,  5.9989e-02,  1.9742e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3430e-02,  4.6935e-02,  2.8419e-02],\n",
      "          [-5.1606e-02,  4.9877e-02, -7.9535e-02],\n",
      "          [ 2.5186e-02, -3.2440e-02,  1.1279e-02]],\n",
      "\n",
      "         [[ 1.4911e-02,  7.2578e-02, -2.0418e-02],\n",
      "          [-7.7740e-02, -7.6620e-02, -1.6479e-02],\n",
      "          [-4.2868e-02,  4.7652e-02, -2.7024e-02]],\n",
      "\n",
      "         [[-5.3361e-02, -1.9322e-05,  7.4700e-02],\n",
      "          [ 6.2697e-02,  7.4712e-02, -3.1695e-02],\n",
      "          [ 5.3407e-02,  1.0158e-03, -5.9842e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0353e-02,  2.1807e-02, -1.6407e-02],\n",
      "          [-3.4404e-02, -7.3076e-02,  6.0079e-02],\n",
      "          [ 2.6091e-02,  2.8053e-02,  7.9564e-02]],\n",
      "\n",
      "         [[-3.3576e-02, -2.2129e-02, -7.6127e-02],\n",
      "          [-2.4709e-02, -5.3727e-02,  5.5421e-02],\n",
      "          [ 3.8477e-02,  9.2502e-03,  1.2810e-02]],\n",
      "\n",
      "         [[ 6.1246e-02, -5.5975e-02,  5.0553e-02],\n",
      "          [-4.1061e-02,  3.1663e-02,  1.5833e-02],\n",
      "          [ 5.8954e-04, -6.6553e-02, -6.0173e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.1347e-02,  7.8966e-02, -1.2516e-02],\n",
      "          [-3.0181e-02, -4.7166e-02,  3.6148e-02],\n",
      "          [-2.8260e-02, -7.1057e-02, -6.4213e-03]],\n",
      "\n",
      "         [[ 3.3559e-02, -1.7080e-02,  4.0446e-03],\n",
      "          [ 4.2313e-02,  7.2629e-02, -1.8229e-02],\n",
      "          [-1.2859e-02, -3.6341e-02, -7.5279e-02]],\n",
      "\n",
      "         [[ 7.3921e-03,  6.4605e-02, -2.8486e-02],\n",
      "          [-8.2110e-02,  3.2614e-02,  6.0510e-02],\n",
      "          [ 4.8894e-02, -2.8136e-02,  1.2750e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7406e-02,  4.5276e-02,  8.2459e-02],\n",
      "          [-4.8289e-02,  2.7903e-03, -3.1251e-03],\n",
      "          [-7.7599e-02, -6.7865e-03,  2.7612e-02]],\n",
      "\n",
      "         [[ 1.1618e-02,  2.9435e-02, -2.0543e-02],\n",
      "          [ 2.1408e-02,  5.8843e-02, -2.0847e-02],\n",
      "          [ 6.7109e-02,  3.1563e-02, -7.4383e-02]],\n",
      "\n",
      "         [[ 3.6369e-02, -2.8389e-04, -3.3936e-02],\n",
      "          [ 5.7114e-02, -2.2839e-02, -4.6399e-02],\n",
      "          [-7.5418e-02, -4.0376e-02, -8.2407e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5982e-02, -5.5451e-02,  7.2990e-02],\n",
      "          [-2.5207e-02,  5.4898e-02,  3.3192e-02],\n",
      "          [-2.4428e-03,  7.4467e-02, -7.9684e-02]],\n",
      "\n",
      "         [[-1.0514e-02, -3.5421e-02, -7.8899e-02],\n",
      "          [ 9.0555e-03,  4.7322e-02,  1.3156e-02],\n",
      "          [-6.3584e-02, -4.2632e-02, -5.9484e-02]],\n",
      "\n",
      "         [[ 3.3517e-02,  7.1615e-02,  3.9741e-02],\n",
      "          [ 2.9035e-02, -4.5686e-02,  5.5914e-02],\n",
      "          [-1.2231e-02,  3.6088e-02,  1.9249e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8645e-02,  7.0219e-03, -5.1752e-02],\n",
      "          [ 1.6244e-02, -4.8879e-02,  3.7625e-02],\n",
      "          [ 1.0774e-02, -4.8348e-02,  2.4331e-02]],\n",
      "\n",
      "         [[ 7.9089e-03, -7.0707e-02,  1.2903e-02],\n",
      "          [-1.5742e-02,  4.1991e-02,  1.5129e-02],\n",
      "          [-4.8816e-02, -3.3365e-02,  3.7623e-03]],\n",
      "\n",
      "         [[ 2.4929e-02, -8.0111e-02, -7.5804e-03],\n",
      "          [ 8.1056e-02, -4.8050e-02, -2.3553e-02],\n",
      "          [ 6.7008e-02,  6.4885e-02,  2.8296e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6281e-03,  6.2192e-02, -3.2472e-02],\n",
      "          [ 6.6346e-02,  6.8932e-02, -2.4220e-02],\n",
      "          [ 7.9220e-02, -6.5945e-02, -7.3164e-02]],\n",
      "\n",
      "         [[-2.2052e-03, -3.0236e-02, -5.9376e-02],\n",
      "          [-6.3337e-02, -7.7832e-02,  6.8149e-02],\n",
      "          [ 5.1204e-02,  3.1085e-02,  3.1564e-02]],\n",
      "\n",
      "         [[-4.2330e-02,  6.7015e-02,  6.4190e-02],\n",
      "          [-4.3444e-02,  1.4168e-02,  7.5238e-02],\n",
      "          [ 5.1036e-02, -3.6417e-02, -3.7279e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1618e-02,  5.7221e-02,  4.1387e-03],\n",
      "          [ 6.5945e-02,  4.9933e-02, -7.5360e-02],\n",
      "          [ 1.7413e-03, -3.1640e-02,  8.2278e-02]],\n",
      "\n",
      "         [[-7.4426e-03,  6.5606e-02, -6.3629e-02],\n",
      "          [ 4.8526e-02,  1.8202e-02,  7.1652e-02],\n",
      "          [-6.0137e-02, -4.4260e-02, -5.6074e-02]],\n",
      "\n",
      "         [[-4.8176e-02, -2.1101e-02, -2.8955e-02],\n",
      "          [ 6.2424e-02,  1.3813e-02, -7.5660e-02],\n",
      "          [ 4.5823e-02, -5.1825e-03, -8.0726e-02]]]])), ('blocks.0.bias', tensor([-0.0662, -0.0048, -0.0737,  0.0792,  0.0494,  0.0610, -0.0202, -0.0325,\n",
      "         0.0233, -0.0539, -0.0826,  0.0776, -0.0812, -0.0186,  0.0462,  0.0219])), ('blocks.1.weight', tensor([[[[ 0.0240,  0.0167, -0.0450],\n",
      "          [ 0.0772, -0.0440,  0.0410],\n",
      "          [ 0.0376, -0.0073,  0.0274]],\n",
      "\n",
      "         [[-0.0287,  0.0022,  0.0541],\n",
      "          [-0.0026, -0.0107,  0.0427],\n",
      "          [-0.0413, -0.0271, -0.0454]],\n",
      "\n",
      "         [[-0.0388, -0.0425, -0.0588],\n",
      "          [ 0.0521, -0.0226,  0.0412],\n",
      "          [-0.0810, -0.0346, -0.0055]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0149, -0.0680,  0.0663],\n",
      "          [-0.0239,  0.0545,  0.0486],\n",
      "          [ 0.0792, -0.0496,  0.0666]],\n",
      "\n",
      "         [[ 0.0595, -0.0361, -0.0452],\n",
      "          [ 0.0542, -0.0354, -0.0532],\n",
      "          [-0.0155, -0.0702,  0.0346]],\n",
      "\n",
      "         [[-0.0822, -0.0594,  0.0197],\n",
      "          [ 0.0649,  0.0093, -0.0252],\n",
      "          [ 0.0411,  0.0427,  0.0177]]],\n",
      "\n",
      "\n",
      "        [[[-0.0239, -0.0188,  0.0398],\n",
      "          [ 0.0449,  0.0099, -0.0160],\n",
      "          [ 0.0596,  0.0705, -0.0014]],\n",
      "\n",
      "         [[ 0.0413,  0.0755, -0.0346],\n",
      "          [ 0.0789,  0.0135,  0.0109],\n",
      "          [ 0.0058, -0.0461, -0.0714]],\n",
      "\n",
      "         [[-0.0738, -0.0326,  0.0712],\n",
      "          [-0.0660, -0.0083,  0.0708],\n",
      "          [ 0.0463,  0.0789,  0.0580]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0325, -0.0767,  0.0713],\n",
      "          [-0.0437, -0.0176,  0.0368],\n",
      "          [ 0.0330,  0.0064, -0.0575]],\n",
      "\n",
      "         [[-0.0377, -0.0251,  0.0307],\n",
      "          [-0.0329,  0.0540,  0.0411],\n",
      "          [ 0.0685,  0.0195, -0.0687]],\n",
      "\n",
      "         [[-0.0802, -0.0441,  0.0685],\n",
      "          [-0.0500, -0.0453, -0.0172],\n",
      "          [-0.0035, -0.0021,  0.0203]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0569, -0.0077, -0.0542],\n",
      "          [-0.0070,  0.0428,  0.0789],\n",
      "          [-0.0297,  0.0554,  0.0434]],\n",
      "\n",
      "         [[ 0.0089,  0.0480,  0.0486],\n",
      "          [-0.0028,  0.0497, -0.0723],\n",
      "          [-0.0179, -0.0092, -0.0716]],\n",
      "\n",
      "         [[-0.0314,  0.0383,  0.0679],\n",
      "          [-0.0045, -0.0564,  0.0681],\n",
      "          [ 0.0153,  0.0568, -0.0516]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0067, -0.0109,  0.0663],\n",
      "          [ 0.0010,  0.0636, -0.0440],\n",
      "          [ 0.0260, -0.0214, -0.0101]],\n",
      "\n",
      "         [[ 0.0718,  0.0148,  0.0823],\n",
      "          [-0.0279,  0.0350, -0.0574],\n",
      "          [ 0.0337, -0.0420,  0.0691]],\n",
      "\n",
      "         [[ 0.0605, -0.0049,  0.0601],\n",
      "          [ 0.0743,  0.0564, -0.0094],\n",
      "          [-0.0552,  0.0159, -0.0506]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0370, -0.0395,  0.0197],\n",
      "          [-0.0726,  0.0406, -0.0584],\n",
      "          [ 0.0697,  0.0093,  0.0528]],\n",
      "\n",
      "         [[-0.0376,  0.0222,  0.0545],\n",
      "          [-0.0419,  0.0634, -0.0122],\n",
      "          [-0.0267, -0.0149, -0.0299]],\n",
      "\n",
      "         [[ 0.0664, -0.0763,  0.0150],\n",
      "          [-0.0403, -0.0220, -0.0612],\n",
      "          [-0.0775,  0.0696,  0.0217]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0808,  0.0586,  0.0774],\n",
      "          [-0.0337, -0.0371,  0.0812],\n",
      "          [ 0.0802,  0.0397,  0.0761]],\n",
      "\n",
      "         [[ 0.0472, -0.0451, -0.0168],\n",
      "          [-0.0402,  0.0249,  0.0433],\n",
      "          [ 0.0526,  0.0501,  0.0410]],\n",
      "\n",
      "         [[ 0.0059,  0.0230,  0.0139],\n",
      "          [-0.0796, -0.0312, -0.0261],\n",
      "          [ 0.0186, -0.0754, -0.0404]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0649,  0.0733,  0.0052],\n",
      "          [-0.0140, -0.0340,  0.0659],\n",
      "          [ 0.0197,  0.0218,  0.0691]],\n",
      "\n",
      "         [[-0.0290, -0.0331, -0.0399],\n",
      "          [-0.0441, -0.0744,  0.0651],\n",
      "          [ 0.0169, -0.0026,  0.0401]],\n",
      "\n",
      "         [[ 0.0591,  0.0608,  0.0506],\n",
      "          [-0.0026,  0.0548,  0.0477],\n",
      "          [ 0.0178, -0.0268,  0.0322]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0726,  0.0447, -0.0639],\n",
      "          [ 0.0063,  0.0528,  0.0183],\n",
      "          [ 0.0059, -0.0609,  0.0381]],\n",
      "\n",
      "         [[-0.0681, -0.0680,  0.0308],\n",
      "          [-0.0301,  0.0182, -0.0088],\n",
      "          [ 0.0008,  0.0012, -0.0774]],\n",
      "\n",
      "         [[ 0.0582,  0.0096,  0.0649],\n",
      "          [-0.0791,  0.0776, -0.0812],\n",
      "          [ 0.0551,  0.0470, -0.0447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0562, -0.0123, -0.0816],\n",
      "          [-0.0206, -0.0614, -0.0527],\n",
      "          [-0.0266,  0.0498,  0.0390]],\n",
      "\n",
      "         [[-0.0249,  0.0316,  0.0726],\n",
      "          [ 0.0235,  0.0097, -0.0473],\n",
      "          [-0.0736, -0.0469, -0.0647]],\n",
      "\n",
      "         [[ 0.0179,  0.0006, -0.0414],\n",
      "          [-0.0749, -0.0008,  0.0416],\n",
      "          [ 0.0738, -0.0457,  0.0644]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0316,  0.0453, -0.0611],\n",
      "          [ 0.0805,  0.0571, -0.0040],\n",
      "          [-0.0543,  0.0158, -0.0356]],\n",
      "\n",
      "         [[ 0.0077,  0.0286, -0.0827],\n",
      "          [-0.0244, -0.0267, -0.0681],\n",
      "          [ 0.0430,  0.0140, -0.0343]],\n",
      "\n",
      "         [[-0.0278, -0.0003, -0.0575],\n",
      "          [ 0.0582, -0.0264, -0.0361],\n",
      "          [ 0.0350, -0.0572, -0.0264]]]])), ('blocks.1.bias', tensor([ 0.0549,  0.0235,  0.0421, -0.0528, -0.0370, -0.0309, -0.0313, -0.0564,\n",
      "        -0.0457, -0.0741,  0.0126, -0.0534,  0.0189,  0.0716, -0.0232, -0.0311,\n",
      "         0.0335,  0.0456,  0.0599,  0.0372,  0.0585, -0.0424, -0.0606, -0.0063,\n",
      "         0.0499,  0.0103,  0.0396, -0.0099,  0.0026, -0.0712,  0.0803,  0.0696])), ('blocks.2.weight', tensor([[[[-0.0509,  0.0061,  0.0098],\n",
      "          [-0.0024,  0.0449,  0.0282],\n",
      "          [ 0.0016, -0.0537, -0.0495]],\n",
      "\n",
      "         [[-0.0255,  0.0367,  0.0271],\n",
      "          [ 0.0073, -0.0249, -0.0020],\n",
      "          [ 0.0089, -0.0114, -0.0514]],\n",
      "\n",
      "         [[ 0.0559, -0.0361,  0.0481],\n",
      "          [-0.0470,  0.0149,  0.0364],\n",
      "          [-0.0078, -0.0411, -0.0499]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0173,  0.0527,  0.0482],\n",
      "          [ 0.0327,  0.0106,  0.0532],\n",
      "          [-0.0321,  0.0179,  0.0143]],\n",
      "\n",
      "         [[ 0.0161,  0.0278, -0.0091],\n",
      "          [ 0.0587, -0.0342,  0.0408],\n",
      "          [-0.0498, -0.0582,  0.0171]],\n",
      "\n",
      "         [[ 0.0333,  0.0472,  0.0117],\n",
      "          [-0.0152, -0.0574,  0.0092],\n",
      "          [-0.0522,  0.0391, -0.0217]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0325, -0.0376,  0.0285],\n",
      "          [ 0.0356,  0.0495,  0.0167],\n",
      "          [-0.0516, -0.0292, -0.0215]],\n",
      "\n",
      "         [[ 0.0016, -0.0171,  0.0246],\n",
      "          [-0.0037,  0.0319, -0.0322],\n",
      "          [-0.0300, -0.0181,  0.0040]],\n",
      "\n",
      "         [[-0.0522, -0.0028,  0.0069],\n",
      "          [-0.0144, -0.0322,  0.0577],\n",
      "          [ 0.0316,  0.0514, -0.0461]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0568, -0.0087, -0.0286],\n",
      "          [-0.0364, -0.0214,  0.0381],\n",
      "          [ 0.0003, -0.0533, -0.0177]],\n",
      "\n",
      "         [[-0.0082, -0.0005, -0.0222],\n",
      "          [-0.0066, -0.0308, -0.0447],\n",
      "          [ 0.0410,  0.0306,  0.0083]],\n",
      "\n",
      "         [[-0.0277, -0.0277, -0.0582],\n",
      "          [ 0.0025,  0.0198,  0.0126],\n",
      "          [-0.0227, -0.0414,  0.0074]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0587, -0.0528, -0.0575],\n",
      "          [-0.0103, -0.0589,  0.0392],\n",
      "          [-0.0260, -0.0084,  0.0327]],\n",
      "\n",
      "         [[-0.0374,  0.0565, -0.0147],\n",
      "          [-0.0390,  0.0121,  0.0011],\n",
      "          [ 0.0445,  0.0583,  0.0553]],\n",
      "\n",
      "         [[-0.0564, -0.0292,  0.0108],\n",
      "          [ 0.0299,  0.0201, -0.0450],\n",
      "          [ 0.0478, -0.0268, -0.0293]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0089, -0.0571, -0.0475],\n",
      "          [ 0.0360,  0.0258,  0.0038],\n",
      "          [ 0.0588, -0.0175, -0.0264]],\n",
      "\n",
      "         [[ 0.0452,  0.0370, -0.0028],\n",
      "          [-0.0039,  0.0083, -0.0178],\n",
      "          [ 0.0225, -0.0074, -0.0131]],\n",
      "\n",
      "         [[-0.0482,  0.0479, -0.0090],\n",
      "          [ 0.0028, -0.0465, -0.0525],\n",
      "          [-0.0121,  0.0563, -0.0346]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0297, -0.0317,  0.0110],\n",
      "          [ 0.0132, -0.0009, -0.0531],\n",
      "          [-0.0235, -0.0185, -0.0354]],\n",
      "\n",
      "         [[-0.0367, -0.0483, -0.0368],\n",
      "          [-0.0031,  0.0435,  0.0137],\n",
      "          [ 0.0381,  0.0391, -0.0544]],\n",
      "\n",
      "         [[-0.0421,  0.0325,  0.0141],\n",
      "          [ 0.0294, -0.0087,  0.0076],\n",
      "          [ 0.0468, -0.0090, -0.0168]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0188,  0.0575, -0.0407],\n",
      "          [-0.0040,  0.0318, -0.0255],\n",
      "          [-0.0369, -0.0398, -0.0355]],\n",
      "\n",
      "         [[-0.0155,  0.0167,  0.0568],\n",
      "          [-0.0019,  0.0478,  0.0293],\n",
      "          [ 0.0101,  0.0444, -0.0331]],\n",
      "\n",
      "         [[-0.0419, -0.0056,  0.0558],\n",
      "          [-0.0288,  0.0336,  0.0434],\n",
      "          [ 0.0129, -0.0252,  0.0093]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0581, -0.0177, -0.0133],\n",
      "          [ 0.0037,  0.0086,  0.0465],\n",
      "          [ 0.0404,  0.0443, -0.0025]],\n",
      "\n",
      "         [[-0.0258,  0.0473,  0.0389],\n",
      "          [ 0.0086, -0.0099,  0.0580],\n",
      "          [ 0.0017, -0.0525, -0.0313]],\n",
      "\n",
      "         [[-0.0364, -0.0230, -0.0489],\n",
      "          [ 0.0468, -0.0087,  0.0455],\n",
      "          [ 0.0389,  0.0588, -0.0559]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0027,  0.0566,  0.0325],\n",
      "          [-0.0004, -0.0291, -0.0203],\n",
      "          [-0.0117,  0.0518, -0.0422]],\n",
      "\n",
      "         [[ 0.0379,  0.0025,  0.0176],\n",
      "          [-0.0246, -0.0186,  0.0523],\n",
      "          [-0.0560, -0.0558,  0.0089]],\n",
      "\n",
      "         [[-0.0088,  0.0255, -0.0581],\n",
      "          [-0.0162,  0.0518,  0.0172],\n",
      "          [ 0.0166,  0.0563, -0.0328]]],\n",
      "\n",
      "\n",
      "        [[[-0.0577, -0.0397,  0.0003],\n",
      "          [ 0.0359, -0.0129,  0.0520],\n",
      "          [ 0.0517, -0.0427,  0.0221]],\n",
      "\n",
      "         [[ 0.0160, -0.0491, -0.0293],\n",
      "          [ 0.0042, -0.0541, -0.0480],\n",
      "          [-0.0584, -0.0478, -0.0290]],\n",
      "\n",
      "         [[ 0.0577,  0.0376, -0.0424],\n",
      "          [-0.0136,  0.0269, -0.0419],\n",
      "          [ 0.0231, -0.0129, -0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0372, -0.0448,  0.0367],\n",
      "          [-0.0189,  0.0156, -0.0472],\n",
      "          [-0.0378, -0.0146,  0.0495]],\n",
      "\n",
      "         [[ 0.0582, -0.0354, -0.0236],\n",
      "          [-0.0039,  0.0434, -0.0371],\n",
      "          [ 0.0470, -0.0171, -0.0194]],\n",
      "\n",
      "         [[ 0.0560,  0.0248,  0.0476],\n",
      "          [-0.0432, -0.0587, -0.0115],\n",
      "          [-0.0373,  0.0099, -0.0585]]]])), ('blocks.2.bias', tensor([ 0.0090, -0.0402, -0.0493, -0.0536,  0.0074,  0.0251,  0.0428,  0.0258,\n",
      "         0.0114, -0.0334,  0.0207,  0.0407, -0.0154,  0.0446,  0.0025,  0.0195,\n",
      "         0.0154, -0.0238,  0.0392,  0.0463,  0.0289,  0.0173, -0.0251, -0.0175,\n",
      "         0.0579,  0.0302,  0.0370, -0.0492,  0.0340, -0.0277,  0.0400, -0.0135])), ('blocks.3.weight', tensor([[[[ 0.0304,  0.0410, -0.0502],\n",
      "          [-0.0326,  0.0191, -0.0505],\n",
      "          [ 0.0066, -0.0154, -0.0088]],\n",
      "\n",
      "         [[-0.0569, -0.0246,  0.0132],\n",
      "          [-0.0314,  0.0571,  0.0521],\n",
      "          [-0.0561,  0.0175,  0.0453]],\n",
      "\n",
      "         [[-0.0135, -0.0337, -0.0185],\n",
      "          [ 0.0401,  0.0453,  0.0196],\n",
      "          [-0.0060, -0.0339,  0.0321]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0475, -0.0582,  0.0042],\n",
      "          [ 0.0300,  0.0553, -0.0214],\n",
      "          [-0.0277,  0.0134,  0.0187]],\n",
      "\n",
      "         [[-0.0462, -0.0003, -0.0135],\n",
      "          [-0.0016, -0.0573, -0.0272],\n",
      "          [-0.0551,  0.0243,  0.0041]],\n",
      "\n",
      "         [[ 0.0304,  0.0498,  0.0567],\n",
      "          [ 0.0431,  0.0240,  0.0027],\n",
      "          [ 0.0134, -0.0548, -0.0245]]],\n",
      "\n",
      "\n",
      "        [[[-0.0046, -0.0421,  0.0059],\n",
      "          [ 0.0411,  0.0408, -0.0051],\n",
      "          [ 0.0143,  0.0437,  0.0239]],\n",
      "\n",
      "         [[-0.0089,  0.0385,  0.0375],\n",
      "          [-0.0396,  0.0321, -0.0521],\n",
      "          [-0.0115,  0.0304,  0.0484]],\n",
      "\n",
      "         [[ 0.0582, -0.0230,  0.0481],\n",
      "          [ 0.0112,  0.0134,  0.0556],\n",
      "          [ 0.0511, -0.0560, -0.0345]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0330, -0.0255, -0.0294],\n",
      "          [-0.0028,  0.0199,  0.0057],\n",
      "          [-0.0242, -0.0534,  0.0119]],\n",
      "\n",
      "         [[-0.0584,  0.0125, -0.0347],\n",
      "          [-0.0383,  0.0553, -0.0261],\n",
      "          [-0.0501,  0.0581,  0.0334]],\n",
      "\n",
      "         [[-0.0452, -0.0064, -0.0420],\n",
      "          [-0.0034, -0.0402, -0.0290],\n",
      "          [ 0.0345, -0.0369, -0.0135]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0357,  0.0005,  0.0069],\n",
      "          [-0.0292, -0.0014, -0.0084],\n",
      "          [ 0.0421, -0.0112, -0.0266]],\n",
      "\n",
      "         [[-0.0286,  0.0306, -0.0481],\n",
      "          [-0.0483,  0.0429, -0.0395],\n",
      "          [ 0.0391,  0.0014, -0.0489]],\n",
      "\n",
      "         [[-0.0168,  0.0070, -0.0303],\n",
      "          [ 0.0122, -0.0136, -0.0052],\n",
      "          [ 0.0187, -0.0214,  0.0375]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0067, -0.0447,  0.0322],\n",
      "          [-0.0413,  0.0357, -0.0393],\n",
      "          [ 0.0329, -0.0369,  0.0213]],\n",
      "\n",
      "         [[ 0.0415,  0.0013, -0.0522],\n",
      "          [ 0.0004, -0.0334, -0.0094],\n",
      "          [-0.0474, -0.0562,  0.0005]],\n",
      "\n",
      "         [[ 0.0100,  0.0445,  0.0159],\n",
      "          [ 0.0572, -0.0078,  0.0294],\n",
      "          [-0.0272, -0.0390,  0.0147]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0431,  0.0462,  0.0121],\n",
      "          [-0.0341, -0.0448,  0.0339],\n",
      "          [ 0.0285,  0.0378,  0.0333]],\n",
      "\n",
      "         [[ 0.0273, -0.0347,  0.0474],\n",
      "          [-0.0221, -0.0075, -0.0346],\n",
      "          [ 0.0151,  0.0055,  0.0022]],\n",
      "\n",
      "         [[-0.0494,  0.0421,  0.0033],\n",
      "          [ 0.0515,  0.0497,  0.0138],\n",
      "          [-0.0244,  0.0079, -0.0380]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0171, -0.0172, -0.0260],\n",
      "          [ 0.0080,  0.0289,  0.0019],\n",
      "          [-0.0260,  0.0090,  0.0012]],\n",
      "\n",
      "         [[-0.0231,  0.0322,  0.0503],\n",
      "          [-0.0295, -0.0097, -0.0256],\n",
      "          [ 0.0005, -0.0391, -0.0171]],\n",
      "\n",
      "         [[-0.0109,  0.0018, -0.0074],\n",
      "          [ 0.0340, -0.0040,  0.0062],\n",
      "          [ 0.0282,  0.0086, -0.0124]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0412,  0.0012, -0.0110],\n",
      "          [-0.0478,  0.0052,  0.0066],\n",
      "          [-0.0523, -0.0553, -0.0402]],\n",
      "\n",
      "         [[-0.0265,  0.0511, -0.0354],\n",
      "          [-0.0188, -0.0468,  0.0120],\n",
      "          [-0.0539,  0.0360, -0.0133]],\n",
      "\n",
      "         [[ 0.0380,  0.0157, -0.0464],\n",
      "          [ 0.0081,  0.0484,  0.0026],\n",
      "          [ 0.0055,  0.0233, -0.0318]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0053,  0.0464,  0.0519],\n",
      "          [-0.0357,  0.0492,  0.0217],\n",
      "          [-0.0534, -0.0125, -0.0099]],\n",
      "\n",
      "         [[-0.0090,  0.0361,  0.0402],\n",
      "          [-0.0499,  0.0046, -0.0574],\n",
      "          [-0.0374, -0.0064,  0.0018]],\n",
      "\n",
      "         [[ 0.0461, -0.0488, -0.0362],\n",
      "          [-0.0189, -0.0300, -0.0431],\n",
      "          [ 0.0340,  0.0253,  0.0229]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0083, -0.0582,  0.0353],\n",
      "          [ 0.0489, -0.0516,  0.0414],\n",
      "          [ 0.0201, -0.0312, -0.0409]],\n",
      "\n",
      "         [[-0.0430,  0.0047, -0.0149],\n",
      "          [-0.0015,  0.0308,  0.0055],\n",
      "          [-0.0009,  0.0582,  0.0017]],\n",
      "\n",
      "         [[-0.0545, -0.0364, -0.0217],\n",
      "          [-0.0208, -0.0069, -0.0235],\n",
      "          [ 0.0173,  0.0216, -0.0169]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0460, -0.0561,  0.0519],\n",
      "          [ 0.0291, -0.0226, -0.0183],\n",
      "          [ 0.0008,  0.0103, -0.0393]],\n",
      "\n",
      "         [[-0.0491,  0.0490,  0.0206],\n",
      "          [ 0.0007,  0.0491, -0.0482],\n",
      "          [-0.0347, -0.0119,  0.0431]],\n",
      "\n",
      "         [[ 0.0538, -0.0477, -0.0147],\n",
      "          [-0.0234,  0.0383, -0.0255],\n",
      "          [-0.0278,  0.0103,  0.0165]]]])), ('blocks.3.bias', tensor([-0.0357,  0.0116,  0.0220, -0.0291,  0.0368,  0.0490,  0.0481, -0.0012,\n",
      "         0.0527,  0.0066, -0.0549, -0.0429, -0.0395, -0.0579, -0.0114, -0.0104,\n",
      "        -0.0492, -0.0271, -0.0131, -0.0121,  0.0035,  0.0201,  0.0466,  0.0297,\n",
      "        -0.0506,  0.0338, -0.0247, -0.0179,  0.0328,  0.0361, -0.0345, -0.0094,\n",
      "        -0.0122, -0.0356, -0.0517, -0.0573, -0.0495,  0.0278,  0.0010, -0.0545,\n",
      "        -0.0371,  0.0162, -0.0381, -0.0493, -0.0324, -0.0464,  0.0084, -0.0135,\n",
      "         0.0140,  0.0061, -0.0080, -0.0054,  0.0242,  0.0007,  0.0114, -0.0359,\n",
      "        -0.0155, -0.0456, -0.0159, -0.0241,  0.0050,  0.0287, -0.0573,  0.0484,\n",
      "        -0.0513, -0.0390,  0.0108,  0.0318, -0.0035, -0.0073, -0.0093, -0.0201,\n",
      "         0.0515, -0.0309, -0.0450,  0.0284,  0.0092, -0.0097, -0.0487, -0.0233,\n",
      "         0.0032, -0.0511,  0.0030,  0.0171, -0.0432,  0.0382,  0.0123, -0.0345,\n",
      "         0.0544, -0.0346, -0.0496,  0.0288, -0.0146,  0.0091, -0.0258, -0.0016])), ('blocks.4.weight', tensor([[[[ 2.5005e-02, -1.2023e-02, -1.4650e-02],\n",
      "          [ 2.0119e-02, -8.6814e-03, -1.7688e-02],\n",
      "          [ 1.8760e-02, -1.2178e-02, -3.1961e-03]],\n",
      "\n",
      "         [[ 2.5056e-02,  1.3740e-03,  1.3720e-02],\n",
      "          [-2.3640e-02,  2.4481e-02, -1.8338e-02],\n",
      "          [ 6.4692e-03, -5.3343e-03,  1.9077e-02]],\n",
      "\n",
      "         [[-1.4208e-02, -1.5027e-02, -3.3067e-02],\n",
      "          [-2.6198e-02,  1.4461e-02,  1.7163e-02],\n",
      "          [-8.9935e-03, -1.8265e-02,  3.3188e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2565e-02,  5.5139e-03,  2.6336e-02],\n",
      "          [ 3.2073e-02, -3.1642e-02,  1.5533e-02],\n",
      "          [ 2.1685e-02,  9.2662e-03,  2.5992e-02]],\n",
      "\n",
      "         [[ 3.2612e-02, -3.1302e-02, -3.3089e-02],\n",
      "          [ 1.0362e-02, -6.8779e-03,  4.7191e-03],\n",
      "          [ 4.0782e-03,  2.4664e-02, -3.8762e-04]],\n",
      "\n",
      "         [[-2.8798e-02,  1.1116e-02, -2.8911e-02],\n",
      "          [-3.2860e-02,  1.1205e-02, -4.2045e-03],\n",
      "          [-2.0565e-02, -1.2480e-02, -2.0295e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0941e-02,  2.2539e-02, -1.5236e-02],\n",
      "          [ 8.7197e-03, -7.0490e-03,  2.7712e-02],\n",
      "          [ 1.6447e-02,  2.6128e-02, -2.1116e-02]],\n",
      "\n",
      "         [[ 2.6043e-02, -3.1368e-02, -3.0025e-02],\n",
      "          [-1.1363e-02, -1.9117e-02, -3.6445e-03],\n",
      "          [ 2.9191e-02, -1.2336e-02, -3.3900e-02]],\n",
      "\n",
      "         [[-3.1544e-02,  5.3329e-03, -1.4489e-02],\n",
      "          [ 2.1001e-02,  5.5504e-03, -2.6855e-02],\n",
      "          [ 2.1956e-02,  3.3880e-02, -2.9746e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8392e-02, -2.2804e-02, -4.2370e-03],\n",
      "          [-1.8508e-03,  3.7100e-03, -1.8178e-02],\n",
      "          [-1.6676e-03, -2.4812e-03,  3.1837e-02]],\n",
      "\n",
      "         [[ 2.7081e-02,  1.5849e-02, -1.3964e-02],\n",
      "          [-3.3200e-02, -1.7302e-02, -1.0569e-02],\n",
      "          [-2.0981e-02, -1.6928e-02, -3.2879e-02]],\n",
      "\n",
      "         [[-5.0574e-03, -7.5059e-03, -1.6628e-02],\n",
      "          [ 2.6377e-02, -2.9064e-02, -2.2749e-02],\n",
      "          [-1.3096e-02,  1.3878e-02, -1.9533e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4706e-02,  6.4438e-03, -7.0394e-03],\n",
      "          [-1.6653e-02,  2.6723e-02, -3.2302e-03],\n",
      "          [ 1.7448e-02,  1.4736e-03,  2.5974e-02]],\n",
      "\n",
      "         [[ 1.0246e-02, -3.3669e-02, -2.4353e-02],\n",
      "          [ 2.7010e-02,  2.6354e-02,  1.5618e-02],\n",
      "          [ 1.3829e-02, -1.9034e-02,  9.6121e-03]],\n",
      "\n",
      "         [[ 1.6538e-02,  1.5640e-02,  2.7820e-02],\n",
      "          [-3.0112e-02, -1.3895e-02,  2.7388e-02],\n",
      "          [-2.4963e-02,  1.2800e-02,  1.2060e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6842e-02,  2.5344e-02, -1.2364e-02],\n",
      "          [-2.9941e-02, -2.5146e-03,  2.6025e-03],\n",
      "          [-2.6130e-03, -1.5059e-02,  1.9439e-02]],\n",
      "\n",
      "         [[-1.6825e-02,  5.6219e-03,  7.6926e-03],\n",
      "          [-2.7506e-02,  5.7262e-03,  3.3332e-02],\n",
      "          [ 1.6432e-02, -1.7945e-02,  2.7998e-02]],\n",
      "\n",
      "         [[ 2.0293e-02,  2.6067e-02,  1.4619e-02],\n",
      "          [-3.2335e-02,  2.6183e-02,  2.3881e-02],\n",
      "          [-9.8341e-03,  1.7669e-02, -1.6141e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.9134e-02,  2.3251e-02, -3.1763e-02],\n",
      "          [ 1.2246e-02, -1.0682e-03,  2.5451e-02],\n",
      "          [ 2.7533e-02,  1.0173e-02,  2.3680e-02]],\n",
      "\n",
      "         [[ 1.0766e-02,  1.7663e-02, -1.0761e-02],\n",
      "          [-3.4121e-03, -1.8062e-02,  8.4353e-03],\n",
      "          [-1.5834e-02,  7.2946e-03, -2.3875e-02]],\n",
      "\n",
      "         [[ 2.1795e-02, -5.4654e-03,  9.0111e-03],\n",
      "          [ 2.1962e-02,  1.2605e-03, -3.2701e-02],\n",
      "          [ 8.9315e-03,  2.2619e-02,  1.2859e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4020e-02, -2.5459e-02,  5.4729e-03],\n",
      "          [ 1.6108e-02,  2.7880e-02,  1.0183e-02],\n",
      "          [-1.9949e-02,  2.3733e-02, -2.1111e-03]],\n",
      "\n",
      "         [[-3.9644e-03,  2.0989e-02,  2.0442e-02],\n",
      "          [-4.6340e-03,  2.9006e-03, -3.0178e-02],\n",
      "          [-1.4277e-02,  1.6511e-02,  4.8414e-03]],\n",
      "\n",
      "         [[ 6.7172e-03, -2.2557e-03, -1.5560e-02],\n",
      "          [ 1.3722e-02,  1.1375e-02,  1.6759e-02],\n",
      "          [-3.0205e-02,  1.4853e-02, -2.5142e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1728e-02,  3.2596e-02, -2.5307e-02],\n",
      "          [ 1.8752e-02, -8.9532e-03, -2.3664e-02],\n",
      "          [-8.5560e-03,  4.7529e-03, -2.4023e-02]],\n",
      "\n",
      "         [[ 1.2381e-02, -1.4873e-03, -2.7578e-02],\n",
      "          [-3.0901e-02,  2.1741e-03,  6.6803e-03],\n",
      "          [-1.5601e-02,  4.2955e-03,  6.3521e-03]],\n",
      "\n",
      "         [[ 9.6820e-03, -1.5235e-02,  2.8687e-02],\n",
      "          [ 2.1104e-02,  3.3938e-02, -2.8103e-02],\n",
      "          [-1.9413e-02,  1.1784e-02, -1.8479e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8260e-02,  1.3169e-02,  4.8723e-03],\n",
      "          [-1.3500e-03,  1.0698e-02, -1.5095e-03],\n",
      "          [-1.6971e-02,  1.2682e-02, -2.2603e-02]],\n",
      "\n",
      "         [[-1.5630e-02, -3.5346e-04,  2.0962e-02],\n",
      "          [ 9.4268e-03, -2.0078e-03, -3.0895e-02],\n",
      "          [-1.1130e-02, -2.3166e-02, -5.1534e-05]],\n",
      "\n",
      "         [[-2.3971e-02, -1.8273e-02,  1.8033e-02],\n",
      "          [-2.0844e-02, -1.6275e-02,  3.0224e-02],\n",
      "          [-1.5279e-02,  1.7067e-02,  2.7453e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0069e-02, -2.8301e-02, -2.3694e-03],\n",
      "          [-1.0829e-03,  2.8419e-02, -3.3475e-02],\n",
      "          [-3.1637e-02,  2.1742e-02,  2.5918e-02]],\n",
      "\n",
      "         [[ 2.3121e-02,  1.1399e-02, -1.9184e-02],\n",
      "          [-8.9076e-03, -2.1141e-02,  2.4666e-02],\n",
      "          [ 1.6870e-02, -4.2729e-04, -7.2559e-03]],\n",
      "\n",
      "         [[ 9.4250e-03,  8.7476e-03,  1.1515e-02],\n",
      "          [ 6.7559e-03,  2.0473e-02,  2.3238e-02],\n",
      "          [ 5.2068e-03,  5.6452e-03, -2.4544e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6543e-02,  6.0050e-03,  2.2060e-03],\n",
      "          [-1.4002e-02,  1.6708e-02,  1.2799e-02],\n",
      "          [-2.0278e-02, -1.7543e-02, -3.2507e-02]],\n",
      "\n",
      "         [[ 1.7343e-03,  1.8421e-02,  2.1601e-02],\n",
      "          [-2.9320e-02,  2.5851e-02, -2.3772e-02],\n",
      "          [-1.2739e-02,  9.6231e-05, -1.1623e-02]],\n",
      "\n",
      "         [[ 2.2609e-03, -2.8024e-02,  1.6950e-02],\n",
      "          [ 1.4489e-02,  2.7717e-02, -2.9655e-02],\n",
      "          [-4.7310e-03,  2.4479e-02,  2.3173e-02]]]])), ('blocks.4.bias', tensor([ 2.1474e-02, -2.1439e-02, -2.3084e-02,  7.0501e-03,  2.3065e-02,\n",
      "        -3.3260e-02,  2.7039e-02,  3.3763e-02, -6.1594e-03, -2.5060e-02,\n",
      "         8.8564e-03,  3.0064e-02, -1.1246e-02, -2.6203e-02,  1.6777e-02,\n",
      "        -2.0402e-02, -3.3965e-02,  2.5093e-02,  1.5688e-02, -1.3778e-02,\n",
      "         1.0051e-02,  3.0211e-03, -4.3910e-03,  8.1905e-04,  1.6925e-02,\n",
      "        -4.4930e-03,  1.2304e-02,  1.0774e-03, -9.2600e-03, -4.5506e-03,\n",
      "         1.5857e-02, -9.9778e-03, -1.6415e-02,  2.9921e-03,  3.0076e-02,\n",
      "        -9.6159e-03,  2.8211e-02,  1.1981e-02,  1.7357e-03, -1.6369e-02,\n",
      "         8.0318e-03,  3.6363e-03,  1.1707e-02,  1.3639e-02,  3.2148e-02,\n",
      "        -2.4266e-02,  3.1081e-02,  2.6554e-02,  3.2493e-02,  1.3917e-02,\n",
      "         1.5282e-02, -2.9505e-02, -2.0250e-02,  3.3577e-02, -9.9776e-03,\n",
      "        -2.9160e-02,  9.7286e-03,  2.7097e-02,  6.5382e-03, -1.2586e-02,\n",
      "        -1.5973e-03, -2.6229e-02,  1.4258e-02, -2.1512e-02,  1.9719e-02,\n",
      "         2.4134e-02,  3.1651e-02, -1.7724e-02, -1.6287e-02,  1.7718e-02,\n",
      "        -1.8965e-02, -1.2522e-02,  5.9841e-03,  6.5234e-05, -3.0731e-03,\n",
      "         9.4048e-03,  3.0606e-02, -2.6779e-02,  1.8840e-02,  2.4769e-02,\n",
      "         5.1571e-03,  1.9915e-02,  2.8821e-02,  6.6512e-03,  5.9183e-03,\n",
      "         1.5051e-02, -1.9485e-02, -1.6205e-02,  1.8131e-02, -1.6972e-02,\n",
      "         9.6631e-04,  3.1360e-02, -9.8914e-03,  2.0946e-02,  1.4493e-02,\n",
      "         1.7983e-02])), ('blocks.5.weight', tensor([[[[ 0.0237, -0.0332,  0.0240],\n",
      "          [-0.0259, -0.0303, -0.0016],\n",
      "          [ 0.0298, -0.0144,  0.0061]],\n",
      "\n",
      "         [[ 0.0195,  0.0083,  0.0291],\n",
      "          [ 0.0202, -0.0048,  0.0211],\n",
      "          [ 0.0288,  0.0016, -0.0107]],\n",
      "\n",
      "         [[ 0.0315, -0.0174,  0.0145],\n",
      "          [-0.0203,  0.0084, -0.0166],\n",
      "          [ 0.0185,  0.0319,  0.0340]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0174,  0.0114,  0.0148],\n",
      "          [-0.0041,  0.0043, -0.0234],\n",
      "          [-0.0042,  0.0053, -0.0254]],\n",
      "\n",
      "         [[ 0.0081, -0.0010, -0.0063],\n",
      "          [ 0.0325,  0.0239,  0.0253],\n",
      "          [ 0.0020, -0.0184, -0.0338]],\n",
      "\n",
      "         [[-0.0020,  0.0107,  0.0324],\n",
      "          [-0.0101,  0.0060,  0.0116],\n",
      "          [ 0.0204, -0.0034, -0.0264]]],\n",
      "\n",
      "\n",
      "        [[[-0.0071,  0.0339,  0.0307],\n",
      "          [-0.0278, -0.0020,  0.0323],\n",
      "          [-0.0076,  0.0080,  0.0288]],\n",
      "\n",
      "         [[-0.0080,  0.0002, -0.0079],\n",
      "          [ 0.0263,  0.0020,  0.0232],\n",
      "          [ 0.0330,  0.0217, -0.0090]],\n",
      "\n",
      "         [[-0.0212, -0.0260,  0.0049],\n",
      "          [-0.0100,  0.0298,  0.0012],\n",
      "          [-0.0031,  0.0064, -0.0197]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0061, -0.0106,  0.0100],\n",
      "          [ 0.0118,  0.0146,  0.0053],\n",
      "          [-0.0326, -0.0261,  0.0175]],\n",
      "\n",
      "         [[-0.0289,  0.0254,  0.0018],\n",
      "          [-0.0149, -0.0204, -0.0035],\n",
      "          [ 0.0200, -0.0156,  0.0063]],\n",
      "\n",
      "         [[ 0.0165,  0.0322, -0.0267],\n",
      "          [-0.0331,  0.0329, -0.0142],\n",
      "          [ 0.0189,  0.0132,  0.0123]]],\n",
      "\n",
      "\n",
      "        [[[-0.0323,  0.0165,  0.0052],\n",
      "          [ 0.0027, -0.0037, -0.0044],\n",
      "          [ 0.0317,  0.0291,  0.0050]],\n",
      "\n",
      "         [[-0.0297,  0.0149, -0.0132],\n",
      "          [-0.0232, -0.0210, -0.0261],\n",
      "          [-0.0307, -0.0212,  0.0316]],\n",
      "\n",
      "         [[ 0.0284,  0.0275, -0.0159],\n",
      "          [ 0.0151,  0.0239, -0.0188],\n",
      "          [ 0.0238, -0.0143, -0.0288]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0209, -0.0161,  0.0161],\n",
      "          [-0.0036, -0.0002, -0.0125],\n",
      "          [ 0.0105,  0.0261, -0.0004]],\n",
      "\n",
      "         [[-0.0035, -0.0119, -0.0054],\n",
      "          [ 0.0225,  0.0123,  0.0167],\n",
      "          [ 0.0315,  0.0136, -0.0022]],\n",
      "\n",
      "         [[-0.0327, -0.0070, -0.0119],\n",
      "          [ 0.0334, -0.0227,  0.0330],\n",
      "          [-0.0131,  0.0230, -0.0048]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0334,  0.0022, -0.0230],\n",
      "          [ 0.0330, -0.0139,  0.0188],\n",
      "          [ 0.0326,  0.0236, -0.0093]],\n",
      "\n",
      "         [[-0.0024,  0.0053,  0.0284],\n",
      "          [-0.0070, -0.0004,  0.0168],\n",
      "          [ 0.0091, -0.0134, -0.0275]],\n",
      "\n",
      "         [[ 0.0088,  0.0270,  0.0045],\n",
      "          [-0.0147, -0.0299,  0.0066],\n",
      "          [ 0.0146, -0.0232,  0.0195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0266, -0.0087, -0.0168],\n",
      "          [-0.0326, -0.0014,  0.0084],\n",
      "          [-0.0340,  0.0203, -0.0062]],\n",
      "\n",
      "         [[-0.0138,  0.0318,  0.0004],\n",
      "          [-0.0155, -0.0012, -0.0180],\n",
      "          [-0.0303, -0.0339, -0.0340]],\n",
      "\n",
      "         [[-0.0263, -0.0177,  0.0280],\n",
      "          [ 0.0319, -0.0233,  0.0130],\n",
      "          [ 0.0331,  0.0209, -0.0170]]],\n",
      "\n",
      "\n",
      "        [[[-0.0051, -0.0252,  0.0237],\n",
      "          [-0.0213, -0.0265,  0.0189],\n",
      "          [ 0.0040, -0.0181,  0.0216]],\n",
      "\n",
      "         [[ 0.0215, -0.0296, -0.0233],\n",
      "          [ 0.0319, -0.0176, -0.0204],\n",
      "          [-0.0242, -0.0259,  0.0258]],\n",
      "\n",
      "         [[ 0.0031,  0.0265, -0.0071],\n",
      "          [-0.0073, -0.0136,  0.0223],\n",
      "          [-0.0318, -0.0148,  0.0214]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0188, -0.0165,  0.0084],\n",
      "          [ 0.0096,  0.0052, -0.0148],\n",
      "          [ 0.0195,  0.0305,  0.0314]],\n",
      "\n",
      "         [[-0.0287,  0.0339,  0.0338],\n",
      "          [ 0.0060,  0.0035, -0.0067],\n",
      "          [ 0.0307,  0.0182,  0.0089]],\n",
      "\n",
      "         [[ 0.0085,  0.0175, -0.0166],\n",
      "          [ 0.0086,  0.0257,  0.0255],\n",
      "          [ 0.0336,  0.0041, -0.0210]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0326, -0.0304,  0.0047],\n",
      "          [-0.0076,  0.0294,  0.0180],\n",
      "          [-0.0064,  0.0270, -0.0120]],\n",
      "\n",
      "         [[-0.0296,  0.0051, -0.0058],\n",
      "          [ 0.0228,  0.0146,  0.0229],\n",
      "          [-0.0283,  0.0182,  0.0182]],\n",
      "\n",
      "         [[ 0.0065,  0.0079,  0.0196],\n",
      "          [-0.0035, -0.0049,  0.0158],\n",
      "          [ 0.0008,  0.0084,  0.0239]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0213, -0.0147,  0.0125],\n",
      "          [-0.0111, -0.0005, -0.0281],\n",
      "          [-0.0332,  0.0141, -0.0113]],\n",
      "\n",
      "         [[ 0.0291,  0.0074, -0.0233],\n",
      "          [-0.0288, -0.0330, -0.0319],\n",
      "          [-0.0230,  0.0142, -0.0278]],\n",
      "\n",
      "         [[-0.0277,  0.0026, -0.0011],\n",
      "          [-0.0138,  0.0331, -0.0027],\n",
      "          [ 0.0335, -0.0259,  0.0194]]]])), ('blocks.5.bias', tensor([ 0.0089, -0.0129,  0.0316, -0.0329, -0.0157, -0.0308, -0.0164, -0.0124,\n",
      "        -0.0017,  0.0095,  0.0235, -0.0058, -0.0262,  0.0315, -0.0234,  0.0109,\n",
      "         0.0002, -0.0181,  0.0090,  0.0061,  0.0091, -0.0155, -0.0299, -0.0014,\n",
      "        -0.0190, -0.0062, -0.0203, -0.0239,  0.0160,  0.0233,  0.0131,  0.0246,\n",
      "         0.0108,  0.0043, -0.0162, -0.0264, -0.0248,  0.0145,  0.0036,  0.0211,\n",
      "         0.0082, -0.0152,  0.0111, -0.0062,  0.0232, -0.0279, -0.0167,  0.0256,\n",
      "         0.0210, -0.0056, -0.0173,  0.0116, -0.0011,  0.0301, -0.0122,  0.0247,\n",
      "        -0.0080, -0.0087, -0.0054, -0.0258, -0.0185,  0.0265, -0.0092, -0.0004,\n",
      "         0.0244, -0.0211,  0.0204, -0.0158, -0.0199,  0.0296,  0.0127, -0.0199,\n",
      "        -0.0214, -0.0026,  0.0087,  0.0004,  0.0205,  0.0302, -0.0219,  0.0226,\n",
      "         0.0254, -0.0051,  0.0279, -0.0036, -0.0271,  0.0186,  0.0174, -0.0109,\n",
      "        -0.0164,  0.0164,  0.0218, -0.0024,  0.0012,  0.0312, -0.0327,  0.0276,\n",
      "         0.0087,  0.0008, -0.0229,  0.0117, -0.0167,  0.0016,  0.0115,  0.0054,\n",
      "         0.0211,  0.0003,  0.0165, -0.0070,  0.0193,  0.0093, -0.0246, -0.0325,\n",
      "         0.0001,  0.0298, -0.0111, -0.0066, -0.0078,  0.0332, -0.0338, -0.0043,\n",
      "         0.0331, -0.0117, -0.0150, -0.0092, -0.0009,  0.0319, -0.0017,  0.0295,\n",
      "         0.0144, -0.0122, -0.0120,  0.0031,  0.0259, -0.0337,  0.0298, -0.0229,\n",
      "         0.0158, -0.0112, -0.0140, -0.0056, -0.0213,  0.0145, -0.0323, -0.0250,\n",
      "        -0.0323, -0.0053, -0.0122,  0.0199,  0.0005, -0.0233, -0.0209,  0.0328,\n",
      "        -0.0074, -0.0088, -0.0004,  0.0209,  0.0293, -0.0303,  0.0229,  0.0303,\n",
      "        -0.0118, -0.0282, -0.0166, -0.0085,  0.0050, -0.0239, -0.0276,  0.0204,\n",
      "        -0.0167,  0.0069, -0.0072, -0.0159, -0.0150,  0.0117, -0.0274, -0.0001,\n",
      "         0.0121, -0.0310, -0.0150, -0.0240,  0.0269, -0.0030,  0.0015, -0.0256,\n",
      "        -0.0279, -0.0003, -0.0280,  0.0014, -0.0041,  0.0097, -0.0333, -0.0298,\n",
      "         0.0210, -0.0042,  0.0273, -0.0074, -0.0269, -0.0245, -0.0052, -0.0116,\n",
      "         0.0318,  0.0315,  0.0325, -0.0181, -0.0291, -0.0323, -0.0221, -0.0185,\n",
      "        -0.0009, -0.0333,  0.0153,  0.0209, -0.0043, -0.0203,  0.0250, -0.0192,\n",
      "         0.0186, -0.0290,  0.0307, -0.0206,  0.0220, -0.0226,  0.0260, -0.0166,\n",
      "        -0.0095, -0.0332, -0.0092, -0.0059,  0.0250,  0.0179, -0.0294, -0.0092,\n",
      "         0.0307,  0.0331, -0.0227, -0.0024, -0.0187,  0.0041,  0.0238,  0.0336,\n",
      "        -0.0119, -0.0206,  0.0187, -0.0067, -0.0113,  0.0055, -0.0331, -0.0307,\n",
      "        -0.0039,  0.0183, -0.0302, -0.0291,  0.0137,  0.0034, -0.0073, -0.0079])), ('conv_out.weight', tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])), ('conv_out.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "print(my_pipeline.unet.controlnet_cond_embedding.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline=my_pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline(\n",
    "    Image.open(\n",
    "        \"/home/aihao/workspace/DeepLearningContent/datasets/images/data/pixiv/dogface/Illustration/_dogface - pixiv__Illustration_,Miku Nakano,,wedding dress,miko clothing,_88734377_p000.jpg\"\n",
    "    ),\n",
    "    Image.open(\n",
    "        \"/home/aihao/workspace/DeepLearningContent/datasets/images/data/pixiv/dogface/Illustration/_dogface - pixiv__Illustration_Genshin Impact,barefoot,Kokomi,Sangonomiya Kokomi,girl,,underwater,sleep-wear,bellybutton,Genshin Impact 10000+ bookmarks_96862960_p000.jpg\"\n",
    "    ),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
